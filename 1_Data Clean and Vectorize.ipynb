{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12dae4b7",
   "metadata": {},
   "source": [
    "# Part I: Data Clean & Vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341efa5b",
   "metadata": {},
   "source": [
    "## A. Compile & Clean DF\n",
    "- Join 16 csv files downloaded from Candid site\n",
    "- Search Refinements:\n",
    "    - Education\n",
    "    - Malawi (Country), Nepal (Country), Senegal (Country), Burkina Faso (Country), Mali (Country), Guatemala (Country), Nicaragua (Country), Haiti (Country)\n",
    "    - Chicago (Illinois, United States), Boston (Massachusetts, United States), Detroit (Michigan, United States), New York (United States), Bridgeport (Connecticut, United States), Oakland (California, United States)\n",
    "    - Adults, Adolescents, Children, Preteens\n",
    "    - At-risk youth, Economically disadvantaged people\n",
    "    - Recipients located in United States (Country)\n",
    "    - Years from 2021 to 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2372163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc8dd14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('csv_files/candid_1.csv', header = [1])[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c3f788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of all 16 candid .csv downloads\n",
    "candid_lst = []\n",
    "count = 1\n",
    "\n",
    "while count < 17:\n",
    "    candid_lst.append('candid_' + str(count) + '.csv')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "937c9226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of candid dfs and concat\n",
    "df_lst = []\n",
    "for i in candid_lst:\n",
    "    df = pd.read_csv('csv_files/' + i, header = [1])[:-2]\n",
    "    df_lst.append(df)\n",
    "df = pd.concat(df_lst, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfd91d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename cols\n",
    "df = df.rename(columns = {'Grantmaker Name':'grantmaker', 'Grantmaker State':'grantmaker_state',\n",
    "                          'Recipient Name':'recipient', 'Recipient City':'recip_city',\n",
    "                          'Recipient State/Country': 'recip_state_cntry',\n",
    "                          'Year Authorized' : 'year', 'Grant Amount': 'amount', \n",
    "                          'Primary Subject': 'subject', 'Support Strategies':'strategy',\n",
    "                          'Description': 'description'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bc0a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop dupes, grants < 5k, grants without descriptions\n",
    "df = df.drop_duplicates()\n",
    "df = df[df['amount'] >= 5000]\n",
    "df = df[df['description'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9baad0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove scientific notation\n",
    "pd.options.display.float_format = '{:.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea85d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format yr\n",
    "df['year'] = df['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48e445dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean recip_city\n",
    "df['recip_city'] = np.where(df.recip_city.str.contains('New Yorik'), 'New York City', df.recip_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "630202e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean subject \n",
    "df['subject'] = df.subject.str.replace(' ','_')\n",
    "df['subject'] = df.subject.str.replace(\"'\",\"\")\n",
    "df['subject'] = df.subject.str.replace(\"-\",\"_\")\n",
    "df['subject'] = df['subject'].apply(lambda x: x[:-1])\n",
    "df['subject'] = df.subject.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "186bc415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.subject.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60e4521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean strategy\n",
    "df['strategy'] = df.strategy.str.replace(' ','_')\n",
    "df['strategy'] = df.strategy.str.replace(\"'\",\"\")\n",
    "df['strategy'] = df.strategy.str.replace(\"-\",\"_\")\n",
    "df['strategy'] = df.strategy.str.replace(\",\",\"_\")\n",
    "df['strategy'] = df.strategy.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93b2c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['strategy'] = df.strategy.str.split(';').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a73bd6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    [research_and_evaluation, capacity_building_an...\n",
       "3    [faculty_and_staff_development, capacity_build...\n",
       "4    [capacity_building_and_technical_assistance, e...\n",
       "6                    [program_support, equal_access, ]\n",
       "7                                  [program_support, ]\n",
       "Name: strategy, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['strategy'][2:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fc6ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean description of html tags\n",
    "clnr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "\n",
    "def cleanhtml(doc):\n",
    "  clean_doc = re.sub(clnr, '', doc)\n",
    "  return clean_doc\n",
    "\n",
    "df['description'] = df['description'].apply(cleanhtml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "031a830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label grants as major or transformative\n",
    "df['gift_type'] = np.where(df.amount < 100000, 'major','transformative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbcfcd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1036, 11)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "193622e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d225f470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grantmaker</th>\n",
       "      <th>grantmaker_state</th>\n",
       "      <th>recipient</th>\n",
       "      <th>recip_city</th>\n",
       "      <th>recip_state_cntry</th>\n",
       "      <th>subject</th>\n",
       "      <th>year</th>\n",
       "      <th>amount</th>\n",
       "      <th>strategy</th>\n",
       "      <th>description</th>\n",
       "      <th>gift_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bloomberg Philanthropies, Inc.</td>\n",
       "      <td>New York</td>\n",
       "      <td>Success Academy Charter Schools</td>\n",
       "      <td>New York City</td>\n",
       "      <td>New York</td>\n",
       "      <td>elementary_and_secondary_education</td>\n",
       "      <td>2022</td>\n",
       "      <td>100000000.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bloomberg Philanthropies announced $100 millio...</td>\n",
       "      <td>transformative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       grantmaker grantmaker_state  \\\n",
       "0  Bloomberg Philanthropies, Inc.         New York   \n",
       "\n",
       "                         recipient     recip_city recip_state_cntry  \\\n",
       "0  Success Academy Charter Schools  New York City          New York   \n",
       "\n",
       "                              subject  year         amount strategy  \\\n",
       "0  elementary_and_secondary_education  2022 100000000.0000      NaN   \n",
       "\n",
       "                                         description       gift_type  \n",
       "0  Bloomberg Philanthropies announced $100 millio...  transformative  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "446940f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle df\n",
    "import pickle\n",
    "with open('grant_df.pickle', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e97fc141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['doc'] = df['doc'].apply(lambda row: row.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46356e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = df['doc'].tolist()\n",
    "#sum([len(d.split(' ')) for d in corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f0b7d",
   "metadata": {},
   "source": [
    "## B. NLP Text Processing\n",
    "\n",
    "*Coding support from spaCy processing chapter of 'Blueprints for Text Analytics Using Python: Machine Learning-Based Solutions for Common Real World (NLP) Applications' published by O'Relly*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52cae80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy lemmetization\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "320db5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df4a2bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdee8096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(nlp):\n",
    "    prefixes = [pattern for pattern in nlp.Defaults.prefixes\n",
    "               if pattern not in ['-','_','#']]\n",
    "    suffixes = [pattern for pattern in nlp.Defaults.suffixes\n",
    "               if pattern not in ['-','_','#']]\n",
    "    infixes = [pattern for pattern in nlp.Defaults.prefixes\n",
    "               if not re.search(pattern, 'xx-xx')]\n",
    "    \n",
    "    return Tokenizer(vocab = nlp.vocab,\n",
    "                     rules = nlp.Defaults.tokenizer_exceptions,\n",
    "                     prefix_search = compile_prefix_regex(prefixes).search,\n",
    "                     suffix_search = compile_suffix_regex(suffixes).search,\n",
    "                     infix_finditer = compile_infix_regex(infixes).finditer, \n",
    "                     token_match = nlp.Defaults.token_match)\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ccd098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lemmas(doc, **kwargs):\n",
    "    return [t.lemma_.lower().strip() for t in textacy.extract.words(doc, **kwargs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "683d8dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_noun_phrases(doc, preceding_pos=['NOUN'], sep='_'):\n",
    "    patterns = []\n",
    "    for pos in preceding_pos:\n",
    "        patterns.append(f'POS:{pos} POS:NOUN:+')\n",
    "    spans = textacy.extract.matches.token_matches(doc, patterns = patterns)\n",
    "    return [sep.join([t.lemma_.lower().strip() for t in s]) for s in spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f603e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(doc, include_types=None, sep='_'):\n",
    "    ents = textacy.extract.entities(doc,\n",
    "                                   include_types=include_types,\n",
    "                                   exclude_types=None,\n",
    "                                   drop_determiners=True,\n",
    "                                   min_freq=1)\n",
    "    return [sep.join([t.lemma_ for t in e]) for e in ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ae7bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nlp(doc):\n",
    "    return {\n",
    "        'lemma': extract_lemmas(doc,\n",
    "                                 exclude_pos = ['PART','PUNCT','DET','PRON','SYM','SPACE','NUM'],\n",
    "                                 filter_stops = True),\n",
    "        'n_gram': extract_noun_phrases(doc, ['ADJ','NOUN']),\n",
    "        'ent': extract_entities(doc, ['GPE','LOC'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aa479ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grant in support of research to assess the economic, health and environmental impacts on food and to develop a more transparent public procurement process in New York State\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lemma': ['grant',\n",
       "  'support',\n",
       "  'research',\n",
       "  'assess',\n",
       "  'economic',\n",
       "  'health',\n",
       "  'environmental',\n",
       "  'impact',\n",
       "  'food',\n",
       "  'develop',\n",
       "  'transparent',\n",
       "  'public',\n",
       "  'procurement',\n",
       "  'process',\n",
       "  'new',\n",
       "  'york',\n",
       "  'state'],\n",
       " 'n_gram': ['environmental_impact',\n",
       "  'public_procurement',\n",
       "  'public_procurement_process',\n",
       "  'procurement_process'],\n",
       " 'ent': ['New_York_State']}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemma function tester\n",
    "\n",
    "te_doc = df['description'][random.randrange(1000)]\n",
    "te_sp_doc = nlp(te_doc)\n",
    "\n",
    "print(te_sp_doc)\n",
    "extract_nlp(te_sp_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c41f2a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2021\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Outdoor Places Grant Application\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lemma function tester pt 2\n",
    "\n",
    "displacy.render(te_sp_doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a51e937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lemma', 'n_gram', 'ent']\n"
     ]
    }
   ],
   "source": [
    "nlp_columns = list(extract_nlp(nlp.make_doc('')).keys())\n",
    "print(nlp_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96a602b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in nlp_columns:\n",
    "    df[col] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30f2eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    docs = nlp.pipe(df['description'][i:i+batch_size])\n",
    "    \n",
    "    for j, doc in enumerate(docs):\n",
    "        for col, values in extract_nlp(doc).items():\n",
    "            df[col].iloc[i+j] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c7a9df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemma'] = df['lemma'].map(str)\n",
    "df['n_gram'] = df['n_gram'].map(str)\n",
    "df['ent'] = df['ent'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "66dfe615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle lemmatized df\n",
    "with open('grant_df_lemma.pickle', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bce9da7",
   "metadata": {},
   "source": [
    "## C. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2555c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(min_df = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "46931c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vecs(token_col_lst, vec=vec):\n",
    "    dict = {}\n",
    "    for col in token_col_lst:\n",
    "        col_vec = vec.fit_transform(df[col])\n",
    "        col_terms = vec.get_feature_names_out()\n",
    "        dict[col] = [col_vec, col_terms]\n",
    "    return dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ce71f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['lemma','n_gram','ent']\n",
    "\n",
    "mtx_terms_dict = get_vecs(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c68f8ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle vec_terms_dict\n",
    "with open('mtx_terms_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(mtx_terms_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (unsupervised)",
   "language": "python",
   "name": "unsupervised"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
